{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1BR3bNFkOldr-tLU5iqFkdxi1D6aVTlmr",
      "authorship_tag": "ABX9TyNgSpBZIadzePPn235qeAIR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SHIVASAI16256/GEN-AI-B-38/blob/main/2303A52488_GEN_AI_ASS_11_1_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sns\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "import os\n",
        "\n",
        "# For reproducibility\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "# Function to build the CNN model with the provided architecture\n",
        "def build_cnn_model(input_shape, num_classes):\n",
        "    model = models.Sequential()\n",
        "\n",
        "    # Convolution Layer 1 + Max Pooling Layer 1\n",
        "    model.add(layers.Conv2D(64, (3, 3), activation='relu', input_shape=input_shape))\n",
        "    model.add(layers.MaxPooling2D((2, 2)))\n",
        "\n",
        "    # Convolution Layer 2 + Max Pooling Layer 2\n",
        "    model.add(layers.Conv2D(128, (3, 3), activation='relu'))\n",
        "    model.add(layers.MaxPooling2D((2, 2)))\n",
        "\n",
        "    # Convolution Layer 3 + Max Pooling Layer 3\n",
        "    model.add(layers.Conv2D(256, (3, 3), activation='relu'))\n",
        "    model.add(layers.MaxPooling2D((2, 2)))\n",
        "\n",
        "    # Flatten the output for the dense layer\n",
        "    model.add(layers.Flatten())\n",
        "\n",
        "    # Dense Layer\n",
        "    model.add(layers.Dense(256, activation='relu'))\n",
        "\n",
        "    # Output layer\n",
        "    model.add(layers.Dense(num_classes, activation='softmax'))\n",
        "\n",
        "    return model\n",
        "\n",
        "# Function to load images from folders\n",
        "def load_cat_dog_data(train_dir, validation_dir, img_height=150, img_width=150, batch_size=32):\n",
        "    print(\"Loading cat and dog dataset...\")\n",
        "\n",
        "    # Data augmentation for training\n",
        "    train_datagen = ImageDataGenerator(\n",
        "        rescale=1./255,\n",
        "        rotation_range=15,\n",
        "        width_shift_range=0.1,\n",
        "        height_shift_range=0.1,\n",
        "        shear_range=0.1,\n",
        "        zoom_range=0.1,\n",
        "        horizontal_flip=True,\n",
        "        fill_mode='nearest'\n",
        "    )\n",
        "\n",
        "    # Only rescaling for validation\n",
        "    validation_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "    # Load training data\n",
        "    train_generator = train_datagen.flow_from_directory(\n",
        "        train_dir,\n",
        "        target_size=(img_height, img_width),\n",
        "        batch_size=batch_size,\n",
        "        class_mode='categorical'\n",
        "    )\n",
        "\n",
        "    # Load validation data\n",
        "    validation_generator = validation_datagen.flow_from_directory(\n",
        "        validation_dir,\n",
        "        target_size=(img_height, img_width),\n",
        "        batch_size=batch_size,\n",
        "        class_mode='categorical'\n",
        "    )\n",
        "\n",
        "    return train_generator, validation_generator\n",
        "\n",
        "# Function to train the model\n",
        "def train_model(model, train_generator, validation_generator, epochs=10):\n",
        "    # Compile model with Adadelta optimizer as specified\n",
        "    model.compile(\n",
        "        optimizer='adadelta',\n",
        "        loss='categorical_crossentropy',\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "\n",
        "    # Train the model\n",
        "    history = model.fit(\n",
        "        train_generator,\n",
        "        epochs=epochs,\n",
        "        validation_data=validation_generator\n",
        "    )\n",
        "\n",
        "    return history\n",
        "\n",
        "# Function to evaluate the model\n",
        "def evaluate_model(model, validation_generator):\n",
        "    # Evaluate the model on the validation data\n",
        "    validation_steps = validation_generator.samples // validation_generator.batch_size\n",
        "    evaluation = model.evaluate(validation_generator, steps=validation_steps)\n",
        "\n",
        "    test_loss, test_acc = evaluation[0], evaluation[1]\n",
        "    print(f\"\\nValidation accuracy: {test_acc:.4f}\")\n",
        "    print(f\"Validation loss: {test_loss:.4f}\")\n",
        "\n",
        "    # Get predictions\n",
        "    validation_steps = validation_generator.samples // validation_generator.batch_size + 1\n",
        "    predictions = model.predict(validation_generator, steps=validation_steps)\n",
        "    predicted_classes = np.argmax(predictions[:validation_generator.samples], axis=1)\n",
        "    true_classes = validation_generator.classes[:validation_generator.samples]\n",
        "\n",
        "    # Compute confusion matrix\n",
        "    cm = confusion_matrix(true_classes, predicted_classes)\n",
        "\n",
        "    return test_acc, test_loss, cm, true_classes, predicted_classes\n",
        "\n",
        "# Function to plot training history\n",
        "def plot_training_history(history):\n",
        "    # Plot training & validation accuracy values\n",
        "    plt.figure(figsize=(12, 5))\n",
        "\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(history.history['accuracy'])\n",
        "    plt.plot(history.history['val_accuracy'])\n",
        "    plt.title('Model accuracy')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.legend(['Train', 'Validation'], loc='upper left')\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(history.history['loss'])\n",
        "    plt.plot(history.history['val_loss'])\n",
        "    plt.title('Model loss')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.legend(['Train', 'Validation'], loc='upper left')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Function to plot confusion matrix\n",
        "def plot_confusion_matrix(cm, class_names):\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)\n",
        "    plt.xlabel('Predicted')\n",
        "    plt.ylabel('True')\n",
        "    plt.title('Confusion Matrix')\n",
        "    plt.show()\n",
        "\n",
        "# Function to create and evaluate alternative architectures\n",
        "def experiment_with_architectures(train_generator, validation_generator, input_shape, num_classes):\n",
        "    architectures = {\n",
        "        \"Original\": {\n",
        "            \"conv_filters\": [64, 128, 256],\n",
        "            \"dense_units\": [256]\n",
        "        },\n",
        "        \"Deeper\": {\n",
        "            \"conv_filters\": [32, 64, 128, 256],\n",
        "            \"dense_units\": [512, 256]\n",
        "        },\n",
        "        \"Wider\": {\n",
        "            \"conv_filters\": [128, 256, 512],\n",
        "            \"dense_units\": [512]\n",
        "        },\n",
        "        \"Balanced\": {\n",
        "            \"conv_filters\": [64, 128, 256, 256],\n",
        "            \"dense_units\": [512, 256]\n",
        "        }\n",
        "    }\n",
        "\n",
        "    results = {}\n",
        "\n",
        "    for name, arch in architectures.items():\n",
        "        print(f\"\\n\\n{'='*50}\")\n",
        "        print(f\"Training architecture: {name}\")\n",
        "        print(f\"{'='*50}\")\n",
        "\n",
        "        model = models.Sequential()\n",
        "\n",
        "        # Add convolutional layers\n",
        "        for i, filters in enumerate(arch[\"conv_filters\"]):\n",
        "            if i == 0:\n",
        "                model.add(layers.Conv2D(filters, (3, 3), activation='relu', input_shape=input_shape))\n",
        "            else:\n",
        "                model.add(layers.Conv2D(filters, (3, 3), activation='relu'))\n",
        "            model.add(layers.MaxPooling2D((2, 2)))\n",
        "\n",
        "        model.add(layers.Flatten())\n",
        "\n",
        "        # Add dense layers\n",
        "        for units in arch[\"dense_units\"]:\n",
        "            model.add(layers.Dense(units, activation='relu'))\n",
        "\n",
        "        # Output layer\n",
        "        model.add(layers.Dense(num_classes, activation='softmax'))\n",
        "\n",
        "        model.summary()\n",
        "\n",
        "        # Train the model\n",
        "        history = train_model(model, train_generator, validation_generator)\n",
        "\n",
        "        # Evaluate the model\n",
        "        validation_steps = validation_generator.samples // validation_generator.batch_size\n",
        "        eval_result = model.evaluate(validation_generator, steps=validation_steps)\n",
        "        test_loss, test_acc = eval_result[0], eval_result[1]\n",
        "        train_acc = history.history['accuracy'][-1]\n",
        "\n",
        "        # Store results\n",
        "        results[name] = {\n",
        "            \"train_accuracy\": train_acc,\n",
        "            \"validation_accuracy\": test_acc,\n",
        "            \"validation_loss\": test_loss\n",
        "        }\n",
        "\n",
        "    # Compare results\n",
        "    print(\"\\n\\n\")\n",
        "    print(\"=\"*80)\n",
        "    print(\"Architecture Comparison\")\n",
        "    print(\"=\"*80)\n",
        "    print(f\"{'Architecture':<15} {'Train Accuracy':<20} {'Validation Accuracy':<20} {'Validation Loss':<15}\")\n",
        "    print(\"-\"*80)\n",
        "\n",
        "    for arch, metrics in results.items():\n",
        "        print(f\"{arch:<15} {metrics['train_accuracy']:.4f}{' '*12} {metrics['validation_accuracy']:.4f}{' '*12} {metrics['validation_loss']:.4f}\")\n",
        "\n",
        "    # Identify the best architecture\n",
        "    best_arch = max(results.items(), key=lambda x: x[1]['validation_accuracy'])\n",
        "    print(\"\\nBest architecture based on validation accuracy:\")\n",
        "    print(f\"{best_arch[0]} with validation accuracy of {best_arch[1]['validation_accuracy']:.4f}\")\n",
        "\n",
        "# Main execution\n",
        "def main():\n",
        "    # Set your dataset directories here\n",
        "    train_dir = '/content/drive/MyDrive/GEN AI/train'  # Directory with 'cat' and 'dog' subdirectories of training images\n",
        "    validation_dir = '/content/drive/MyDrive/GEN AI/validation-20250404T145130Z-001'  # Directory with 'cat' and 'dog' subdirectories of validation images\n",
        "\n",
        "    img_height = 150\n",
        "    img_width = 150\n",
        "    batch_size = 32\n",
        "    epochs = 10\n",
        "\n",
        "    # Check if directories exist\n",
        "    if not os.path.exists(train_dir):\n",
        "        print(f\"Error: Training directory '{train_dir}' not found!\")\n",
        "        return\n",
        "    if not os.path.exists(validation_dir):\n",
        "        print(f\"Error: Validation directory '{validation_dir}' not found!\")\n",
        "        return\n",
        "\n",
        "    # Load data\n",
        "    train_generator, validation_generator = load_cat_dog_data(\n",
        "        train_dir, validation_dir, img_height, img_width, batch_size\n",
        "    )\n",
        "\n",
        "    # Set input shape and number of classes\n",
        "    input_shape = (img_height, img_width, 3)\n",
        "    num_classes = len(train_generator.class_indices)  # Should be 2 for cats and dogs\n",
        "\n",
        "    print(f\"Number of classes detected: {num_classes}\")\n",
        "    print(f\"Class mapping: {train_generator.class_indices}\")\n",
        "\n",
        "    # Build the CNN model with the provided architecture\n",
        "    print(\"\\nBuilding the CNN model...\")\n",
        "    model = build_cnn_model(input_shape, num_classes)\n",
        "    model.summary()\n",
        "\n",
        "    # Train the model\n",
        "    print(\"\\nTraining the model...\")\n",
        "    history = train_model(model, train_generator, validation_generator, epochs=epochs)\n",
        "\n",
        "    # Evaluate the model\n",
        "    print(\"\\nEvaluating the model...\")\n",
        "    test_acc, test_loss, cm, true_classes, pred_classes = evaluate_model(model, validation_generator)\n",
        "\n",
        "    # Plot training history\n",
        "    plot_training_history(history)\n",
        "\n",
        "    # Plot confusion matrix\n",
        "    class_names = list(train_generator.class_indices.keys())\n",
        "    plot_confusion_matrix(cm, class_names)\n",
        "\n",
        "    # Save the model\n",
        "    model.save('cat_dog_cnn_model.h5')\n",
        "    print(\"Model saved as 'cat_dog_cnn_model.h5'\")\n",
        "\n",
        "    # Experiment with different architectures\n",
        "    print(\"\\nExperimenting with different architectures...\")\n",
        "    experiment_with_architectures(train_generator, validation_generator, input_shape, num_classes)\n",
        "\n",
        "if __name__ == \"_main_\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "VDqEBfDnnvTc"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sns\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "import os\n",
        "\n",
        "# For reproducibility\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "# Function to build the CNN model with the provided architecture\n",
        "def build_cnn_model(input_shape, num_classes):\n",
        "    model = models.Sequential()\n",
        "\n",
        "    # Convolution Layer 1 + Max Pooling Layer 1\n",
        "    model.add(layers.Conv2D(64, (3, 3), activation='relu', input_shape=input_shape))\n",
        "    model.add(layers.MaxPooling2D((2, 2)))\n",
        "\n",
        "    # Convolution Layer 2 + Max Pooling Layer 2\n",
        "    model.add(layers.Conv2D(128, (3, 3), activation='relu'))\n",
        "    model.add(layers.MaxPooling2D((2, 2)))\n",
        "\n",
        "    # Convolution Layer 3 + Max Pooling Layer 3\n",
        "    model.add(layers.Conv2D(256, (3, 3), activation='relu'))\n",
        "    model.add(layers.MaxPooling2D((2, 2)))\n",
        "\n",
        "    # Flatten the output for the dense layer\n",
        "    model.add(layers.Flatten())\n",
        "\n",
        "    # Dense Layer\n",
        "    model.add(layers.Dense(256, activation='relu'))\n",
        "\n",
        "    # Output layer\n",
        "    model.add(layers.Dense(num_classes, activation='softmax'))\n",
        "\n",
        "    return model\n",
        "\n",
        "# Function to load images from folders\n",
        "def load_cat_dog_data(train_dir, validation_dir, img_height=150, img_width=150, batch_size=32):\n",
        "    print(\"Loading cat and dog dataset...\")\n",
        "\n",
        "    # Data augmentation for training\n",
        "    train_datagen = ImageDataGenerator(\n",
        "        rescale=1./255,\n",
        "        rotation_range=15,\n",
        "        width_shift_range=0.1,\n",
        "        height_shift_range=0.1,\n",
        "        shear_range=0.1,\n",
        "        zoom_range=0.1,\n",
        "        horizontal_flip=True,\n",
        "        fill_mode='nearest'\n",
        "    )\n",
        "\n",
        "    # Only rescaling for validation\n",
        "    validation_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "    # Load training data\n",
        "    train_generator = train_datagen.flow_from_directory(\n",
        "        train_dir,\n",
        "        target_size=(img_height, img_width),\n",
        "        batch_size=batch_size,\n",
        "        class_mode='categorical'\n",
        "    )\n",
        "\n",
        "    # Load validation data\n",
        "    validation_generator = validation_datagen.flow_from_directory(\n",
        "        validation_dir,\n",
        "        target_size=(img_height, img_width),\n",
        "        batch_size=batch_size,\n",
        "        class_mode='categorical'\n",
        "    )\n",
        "\n",
        "    return train_generator, validation_generator\n",
        "\n",
        "# Function to train the model\n",
        "def train_model(model, train_generator, validation_generator, epochs=10):\n",
        "    # Compile model with Adadelta optimizer as specified\n",
        "    model.compile(\n",
        "        optimizer='adadelta',\n",
        "        loss='categorical_crossentropy',\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "\n",
        "    # Train the model\n",
        "    history = model.fit(\n",
        "        train_generator,\n",
        "        epochs=epochs,\n",
        "        validation_data=validation_generator\n",
        "    )\n",
        "\n",
        "    return history\n",
        "\n",
        "# Function to evaluate the model\n",
        "def evaluate_model(model, validation_generator):\n",
        "    # Evaluate the model on the validation data\n",
        "    validation_steps = validation_generator.samples // validation_generator.batch_size\n",
        "    evaluation = model.evaluate(validation_generator, steps=validation_steps)\n",
        "\n",
        "    test_loss, test_acc = evaluation[0], evaluation[1]\n",
        "    print(f\"\\nValidation accuracy: {test_acc:.4f}\")\n",
        "    print(f\"Validation loss: {test_loss:.4f}\")\n",
        "\n",
        "    # Get predictions\n",
        "    validation_steps = validation_generator.samples // validation_generator.batch_size + 1\n",
        "    predictions = model.predict(validation_generator, steps=validation_steps)\n",
        "    predicted_classes = np.argmax(predictions[:validation_generator.samples], axis=1)\n",
        "    true_classes = validation_generator.classes[:validation_generator.samples]\n",
        "\n",
        "    # Compute confusion matrix\n",
        "    cm = confusion_matrix(true_classes, predicted_classes)\n",
        "\n",
        "    return test_acc, test_loss, cm, true_classes, predicted_classes\n",
        "\n",
        "# Function to plot training history\n",
        "def plot_training_history(history):\n",
        "    # Plot training & validation accuracy values\n",
        "    plt.figure(figsize=(12, 5))\n",
        "\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(history.history['accuracy'])\n",
        "    plt.plot(history.history['val_accuracy'])\n",
        "    plt.title('Model accuracy')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.legend(['Train', 'Validation'], loc='upper left')\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(history.history['loss'])\n",
        "    plt.plot(history.history['val_loss'])\n",
        "    plt.title('Model loss')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.legend(['Train', 'Validation'], loc='upper left')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Function to plot confusion matrix\n",
        "def plot_confusion_matrix(cm, class_names):\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)\n",
        "    plt.xlabel('Predicted')\n",
        "    plt.ylabel('True')\n",
        "    plt.title('Confusion Matrix')\n",
        "    plt.show()\n",
        "\n",
        "# Function to create and evaluate alternative architectures\n",
        "def experiment_with_architectures(train_generator, validation_generator, input_shape, num_classes):\n",
        "    architectures = {\n",
        "        \"Original\": {\n",
        "            \"conv_filters\": [64, 128, 256],\n",
        "            \"dense_units\": [256]\n",
        "        },\n",
        "        \"Deeper\": {\n",
        "            \"conv_filters\": [32, 64, 128, 256],\n",
        "            \"dense_units\": [512, 256]\n",
        "        },\n",
        "        \"Wider\": {\n",
        "            \"conv_filters\": [128, 256, 512],\n",
        "            \"dense_units\": [512]\n",
        "        },\n",
        "        \"Balanced\": {\n",
        "            \"conv_filters\": [64, 128, 256, 256],\n",
        "            \"dense_units\": [512, 256]\n",
        "        }\n",
        "    }\n",
        "\n",
        "    results = {}\n",
        "\n",
        "    for name, arch in architectures.items():\n",
        "        print(f\"\\n\\n{'='*50}\")\n",
        "        print(f\"Training architecture: {name}\")\n",
        "        print(f\"{'='*50}\")\n",
        "\n",
        "        model = models.Sequential()\n",
        "\n",
        "        # Add convolutional layers\n",
        "        for i, filters in enumerate(arch[\"conv_filters\"]):\n",
        "            if i == 0:\n",
        "                model.add(layers.Conv2D(filters, (3, 3), activation='relu', input_shape=input_shape))\n",
        "            else:\n",
        "                model.add(layers.Conv2D(filters, (3, 3), activation='relu'))\n",
        "            model.add(layers.MaxPooling2D((2, 2)))\n",
        "\n",
        "        model.add(layers.Flatten())\n",
        "\n",
        "        # Add dense layers\n",
        "        for units in arch[\"dense_units\"]:\n",
        "            model.add(layers.Dense(units, activation='relu'))\n",
        "\n",
        "        # Output layer\n",
        "        model.add(layers.Dense(num_classes, activation='softmax'))\n",
        "\n",
        "        model.summary()\n",
        "\n",
        "        # Train the model\n",
        "        history = train_model(model, train_generator, validation_generator)\n",
        "\n",
        "        # Evaluate the model\n",
        "        validation_steps = validation_generator.samples // validation_generator.batch_size\n",
        "        eval_result = model.evaluate(validation_generator, steps=validation_steps)\n",
        "        test_loss, test_acc = eval_result[0], eval_result[1]\n",
        "        train_acc = history.history['accuracy'][-1]\n",
        "\n",
        "        # Store results\n",
        "        results[name] = {\n",
        "            \"train_accuracy\": train_acc,\n",
        "            \"validation_accuracy\": test_acc,\n",
        "            \"validation_loss\": test_loss\n",
        "        }\n",
        "\n",
        "    # Compare results\n",
        "    print(\"\\n\\n\")\n",
        "    print(\"=\"*80)\n",
        "    print(\"Architecture Comparison\")\n",
        "    print(\"=\"*80)\n",
        "    print(f\"{'Architecture':<15} {'Train Accuracy':<20} {'Validation Accuracy':<20} {'Validation Loss':<15}\")\n",
        "    print(\"-\"*80)\n",
        "\n",
        "    for arch, metrics in results.items():\n",
        "        print(f\"{arch:<15} {metrics['train_accuracy']:.4f}{' '*12} {metrics['validation_accuracy']:.4f}{' '*12} {metrics['validation_loss']:.4f}\")\n",
        "\n",
        "    # Identify the best architecture\n",
        "    best_arch = max(results.items(), key=lambda x: x[1]['validation_accuracy'])\n",
        "    print(\"\\nBest architecture based on validation accuracy:\")\n",
        "    print(f\"{best_arch[0]} with validation accuracy of {best_arch[1]['validation_accuracy']:.4f}\")\n",
        "\n",
        "# Main execution\n",
        "def main():\n",
        "    # Set your dataset directories here\n",
        "    train_dir = '/content/drive/MyDrive/train-20250404T145129Z-001/train'  # Directory with 'cat' and 'dog' subdirectories of training images\n",
        "    validation_dir = '/content/drive/MyDrive/validation-20250404T145130Z-001/validation'  # Directory with 'cat' and 'dog' subdirectories of validation images\n",
        "\n",
        "    img_height = 150\n",
        "    img_width = 150\n",
        "    batch_size = 32\n",
        "    epochs = 10\n",
        "\n",
        "    # Check if directories exist\n",
        "    if not os.path.exists(train_dir):\n",
        "        print(f\"Error: Training directory '{train_dir}' not found!\")\n",
        "        return\n",
        "    if not os.path.exists(validation_dir):\n",
        "        print(f\"Error: Validation directory '{validation_dir}' not found!\")\n",
        "        return\n",
        "\n",
        "    # Load data\n",
        "    train_generator, validation_generator = load_cat_dog_data(\n",
        "        train_dir, validation_dir, img_height, img_width, batch_size\n",
        "    )\n",
        "\n",
        "    # Set input shape and number of classes\n",
        "    input_shape = (img_height, img_width, 3)\n",
        "    num_classes = len(train_generator.class_indices)  # Should be 2 for cats and dogs\n",
        "\n",
        "    print(f\"Number of classes detected: {num_classes}\")\n",
        "    print(f\"Class mapping: {train_generator.class_indices}\")\n",
        "\n",
        "    # Build the CNN model with the provided architecture\n",
        "    print(\"\\nBuilding the CNN model...\")\n",
        "    model = build_cnn_model(input_shape, num_classes)\n",
        "    model.summary()\n",
        "\n",
        "    # Train the model\n",
        "    print(\"\\nTraining the model...\")\n",
        "    history = train_model(model, train_generator, validation_generator, epochs=epochs)\n",
        "\n",
        "    # Evaluate the model\n",
        "    print(\"\\nEvaluating the model...\")\n",
        "    test_acc, test_loss, cm, true_classes, pred_classes = evaluate_model(model, validation_generator)\n",
        "\n",
        "    # Plot training history\n",
        "    plot_training_history(history)\n",
        "\n",
        "    # Plot confusion matrix\n",
        "    class_names = list(train_generator.class_indices.keys())\n",
        "    plot_confusion_matrix(cm, class_names)\n",
        "\n",
        "    # Save the model\n",
        "    model.save('cat_dog_cnn_model.h5')\n",
        "    print(\"Model saved as 'cat_dog_cnn_model.h5'\")\n",
        "\n",
        "    # Experiment with different architectures\n",
        "    print(\"\\nExperimenting with different architectures...\")\n",
        "    experiment_with_architectures(train_generator, validation_generator, input_shape, num_classes)\n",
        "\n",
        "if __name__ == \"_main_\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "Bih1_tjHpPwW"
      },
      "execution_count": 4,
      "outputs": []
    }
  ]
}