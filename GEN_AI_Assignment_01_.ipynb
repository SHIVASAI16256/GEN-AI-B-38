{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMdDgyXRjVFZsuQufw1vIeH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SHIVASAI16256/GEN-AI-B-38/blob/main/GEN_AI_Assignment_01_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "q1. (1 ponto) Write Python code from scratch to find error metrics of deep learning model. Actual\n",
        "values and deep learning model predicted values are shown in Table 1. Also compare the results\n",
        "with the outcomes of libraries\n",
        "YActual YP red\n",
        "20 20.5\n",
        "30 30.3\n",
        "40 40.2\n",
        "50 50.6\n",
        "60 60.7\n",
        "Tabela 1: YActual Vs. YP red"
      ],
      "metadata": {
        "id": "G6wvsiUT-nHQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MKjeCNoj5jdd",
        "outputId": "39576b4d-2cc2-4644-9767-a1564230505c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error Metrics Calculated From Scratch:\n",
            "MAE: 0.4600000000000016\n",
            "MSE: 0.24600000000000147\n",
            "RMSE: 0.49598387070549127\n",
            "\n",
            "Error Metrics Using sklearn:\n",
            "MAE: 0.4600000000000016\n",
            "MSE: 0.24600000000000147\n",
            "RMSE: 0.49598387070549127\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
        "\n",
        "YActual = np.array([20, 30, 40, 50, 60])\n",
        "YPred = np.array([20.5, 30.3, 40.2, 50.6, 60.7])\n",
        "\n",
        "def calculate_error_metrics(y_actual, y_pred):\n",
        "    mae = np.mean(np.abs(y_actual - y_pred))\n",
        "    mse = np.mean((y_actual - y_pred) ** 2)\n",
        "    rmse = np.sqrt(mse)\n",
        "    return mae, mse, rmse\n",
        "\n",
        "mae_scratch, mse_scratch, rmse_scratch = calculate_error_metrics(YActual, YPred)\n",
        "\n",
        "mae_sklearn = mean_absolute_error(YActual, YPred)\n",
        "mse_sklearn = mean_squared_error(YActual, YPred)\n",
        "rmse_sklearn = np.sqrt(mse_sklearn)\n",
        "\n",
        "print(\"Error Metrics Calculated From Scratch:\")\n",
        "print(f\"MAE: {mae_scratch}\")\n",
        "print(f\"MSE: {mse_scratch}\")\n",
        "print(f\"RMSE: {rmse_scratch}\")\n",
        "\n",
        "print(\"\\nError Metrics Using sklearn:\")\n",
        "print(f\"MAE: {mae_sklearn}\")\n",
        "print(f\"MSE: {mse_sklearn}\")\n",
        "print(f\"RMSE: {rmse_sklearn}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "SvM3KUtA-koz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. (1 ponto) Write python code from scratch to find evaluation metrics of deep learning model.\n",
        "Actual values and deep learning model predicted values are shown in Table 2. Also compare the\n",
        "results with outcome of libraries\n",
        "YActual YP red\n",
        "0 0 1 1 2 0\n",
        "0 0 1 0 2 0\n",
        "0 1 1 2 2 1\n",
        "0 2 1 0 2 2\n",
        "0 2 1 2 2 2\n",
        "Tabela 2: YActual Vs. YP red\n",
        "• Expected Leaning Outcomes from this assignment related to python\n",
        "– Students are able to understand deep learning model metrics\n",
        "– Students are able to write code in python to find deep learning model metrics\n",
        "– Students are able to use python libraries to find deep learning model metrics\n",
        "• Naming cinvention\n",
        "– Report File Name: RollNo_Week No._Assignment No."
      ],
      "metadata": {
        "id": "z_Em_-lw-yXf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "actual = np.array([\n",
        "    [0, 0, 1, 1, 2, 0],\n",
        "    [0, 0, 1, 0, 2, 0],\n",
        "    [0, 1, 1, 2, 2, 1],\n",
        "    [0, 2, 1, 0, 2, 2],\n",
        "    [0, 2, 1, 2, 2, 2]\n",
        "])\n",
        "predicted = np.array([\n",
        "    [0, 0, 1, 1, 2, 0],\n",
        "    [0, 0, 1, 0, 2, 0],\n",
        "    [0, 1, 1, 2, 2, 1],\n",
        "    [0, 2, 1, 0, 2, 2],\n",
        "    [0, 2, 1, 2, 2, 2]\n",
        "])\n",
        "\n",
        "actual_flat = actual.flatten()\n",
        "predicted_flat = predicted.flatten()\n",
        "\n",
        "accuracy_manual = np.sum(actual_flat == predicted_flat) / len(actual_flat)\n",
        "\n",
        "unique_classes = np.unique(actual_flat)\n",
        "precision_manual = []\n",
        "recall_manual = []\n",
        "f1_manual = []\n",
        "\n",
        "for cls in unique_classes:\n",
        "    tp = np.sum((actual_flat == cls) & (predicted_flat == cls))\n",
        "    fp = np.sum((actual_flat != cls) & (predicted_flat == cls))\n",
        "    fn = np.sum((actual_flat == cls) & (predicted_flat != cls))\n",
        "\n",
        "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
        "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
        "    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
        "\n",
        "    precision_manual.append(precision)\n",
        "    recall_manual.append(recall)\n",
        "    f1_manual.append(f1)\n",
        "\n",
        "accuracy_sklearn = accuracy_score(actual_flat, predicted_flat)\n",
        "precision_sklearn = precision_score(actual_flat, predicted_flat, average=None)\n",
        "recall_sklearn = recall_score(actual_flat, predicted_flat, average=None)\n",
        "f1_sklearn = f1_score(actual_flat, predicted_flat, average=None)\n",
        "\n",
        "print(\"Manual Calculations:\")\n",
        "print(f\"Accuracy: {accuracy_manual}\")\n",
        "print(f\"Precision: {precision_manual}\")\n",
        "print(f\"Recall: {recall_manual}\")\n",
        "print(f\"F1-Score: {f1_manual}\")\n",
        "\n",
        "print(\"\\nUsing sklearn:\")\n",
        "print(f\"Accuracy: {accuracy_sklearn}\")\n",
        "print(f\"Precision: {precision_sklearn}\")\n",
        "print(f\"Recall: {recall_sklearn}\")\n",
        "print(f\"F1-Score: {f1_sklearn}\")\n",
        "\n",
        "if (np.isclose(accuracy_manual, accuracy_sklearn) and\n",
        "    np.allclose(precision_manual, precision_sklearn) and\n",
        "    np.allclose(recall_manual, recall_sklearn) and\n",
        "    np.allclose(f1_manual, f1_sklearn)):\n",
        "    print(\"\\nManual calculations match the library results!\")\n",
        "else:\n",
        "    print(\"\\nThere is a discrepancy between manual calculations and library results.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AVEDe3xSAuov",
        "outputId": "189c3c68-b4fa-4103-9174-111587ae08a3"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Manual Calculations:\n",
            "Accuracy: 1.0\n",
            "Precision: [1.0, 1.0, 1.0]\n",
            "Recall: [1.0, 1.0, 1.0]\n",
            "F1-Score: [1.0, 1.0, 1.0]\n",
            "\n",
            "Using sklearn:\n",
            "Accuracy: 1.0\n",
            "Precision: [1. 1. 1.]\n",
            "Recall: [1. 1. 1.]\n",
            "F1-Score: [1. 1. 1.]\n",
            "\n",
            "Manual calculations match the library results!\n"
          ]
        }
      ]
    }
  ]
}